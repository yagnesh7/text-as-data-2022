{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\yagne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     C:\\Users\\yagne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yagne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\yagne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\yagne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yagne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "C:\\Users\\yagne\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:143: FutureWarning: The sklearn.semi_supervised.label_propagation module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.semi_supervised. Anything that cannot be imported from sklearn.semi_supervised is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\yagne\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator LabelPropagation from version 0.18 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "for dependency in (\"brown\", \"names\", \"wordnet\", \"averaged_perceptron_tagger\", \"universal_tagset\", \"stopwords\"):\n",
    "    nltk.download(dependency)\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    " # https://github.com/EFord36/normalise\n",
    "import normalise\n",
    "import contractions\n",
    "import re\n",
    "import string\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"lowercase\": True,\n",
    "    \"stopwords\": False,\n",
    "    \"normaliser\": False,\n",
    "    \"remove_break_lines\": True,\n",
    "    \"expand_contractions\": True,\n",
    "    \"remove_punctuation\": True,\n",
    "    \"remove_redditchars\": True,\n",
    "    \"remove_urls\": True,\n",
    "    \"lemmatize\": False,\n",
    "    \"stem\": False\n",
    "}\n",
    "translate_table = dict((ord(char), None) for char in string.punctuation + '‘’')   \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "def preprocess(text, args=args, out=\"\"):\n",
    "    clean = text\n",
    "    \n",
    "    # Lowercase\n",
    "    if args[\"lowercase\"]:\n",
    "        clean = clean.lower()\n",
    "\n",
    "    # Remove /r/depression\n",
    "    clean = re.sub(r'\\/?r\\/depression', '', clean)\n",
    "    clean = re.sub(r'rdepression', '', clean)\n",
    "\n",
    "    # Remove URLs\n",
    "    if args[\"remove_urls\"]:\n",
    "        clean = re.sub(r'https?:\\/\\/\\S+', '', clean)\n",
    "\n",
    "    clean = clean.split()\n",
    "\n",
    "    # Contraction Expansion\n",
    "    if args[\"expand_contractions\"]:\n",
    "        clean = [contractions.fix(w) for w in clean]\n",
    "\n",
    "    # Remove reddit specific characters\n",
    "    if args[\"remove_redditchars\"]:\n",
    "        clean = [re.sub(r'\\/r\\/|\\/u\\/',\"\",w) for w in clean]\n",
    "\n",
    "    # Remove punctuation\n",
    "    if args[\"remove_punctuation\"]:\n",
    "        clean = [w.replace(\"-\",\" \") for w in clean]\n",
    "        clean = [w.translate(translate_table) for w in clean]\n",
    "\n",
    "    # Stopwords\n",
    "    if args[\"stopwords\"]:\n",
    "        clean = [w for w in clean if w not in stopwords.words(\"english\")]\n",
    "\n",
    "    # Use normaliser package\n",
    "    if args[\"normaliser\"]:\n",
    "        clean = normalise.normalise(clean, variety=\"AmE\", verbose=False)\n",
    "\n",
    "    # Lemmatizer\n",
    "    if args[\"lemmatize\"]:\n",
    "        clean = [lemmatizer.lemmatize(w) for w in clean]\n",
    "\n",
    "    if args[\"stem\"]:\n",
    "        clean = [stemmer.stem(w) for w in clean]\n",
    "\n",
    "    # Final clean to remove any empty strings\n",
    "    clean = \" \".join(clean).split()\n",
    "    clean = [w for w in clean if w != \"\"]\n",
    "\n",
    "    # Output\n",
    "    if out==\"tokens\":\n",
    "        return clean\n",
    "    else:\n",
    "        clean = \" \".join(clean)\n",
    "        return clean\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/all_dep_data.csv\")\n",
    "data[\"full_text\"] = data.apply(lambda x: x[\"title\"] + \" \" + x[\"selftext\"],axis=1)\n",
    "data[\"clean_text\"] = list(map(preprocess, data[\"full_text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model, feature_extraction, model_selection, naive_bayes, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((31315,), (3480,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(data[\"clean_text\"], data[\"depressed\"], test_size=0.10, random_state=42)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_docfreq = 10\n",
    "ngram_range = (1,3)\n",
    "cv = feature_extraction.text.CountVectorizer(min_df=min_docfreq, ngram_range=ngram_range, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(binary=True, min_df=10, ngram_range=(1, 3))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cv = cv.fit_transform(X_train)\n",
    "X_test_cv = cv.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<31315x108175 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 7202037 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb = naive_bayes.BernoulliNB()\n",
    "bnb.fit(X_train_cv,y_train)\n",
    "y_pred = bnb.predict(X_test_cv)\n",
    "feature_probs = bnb.feature_log_prob_[1,:]\n",
    "feature_names = cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.50      0.54      1760\n",
      "           1       0.55      0.62      0.59      1720\n",
      "\n",
      "    accuracy                           0.56      3480\n",
      "   macro avg       0.57      0.56      0.56      3480\n",
      "weighted avg       0.57      0.56      0.56      3480\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print((metrics.classification_report(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>neg_proba_log</th>\n",
       "      <th>pos_proba_log</th>\n",
       "      <th>log_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22869</th>\n",
       "      <td>depression she</td>\n",
       "      <td>-9.6531</td>\n",
       "      <td>-6.95648</td>\n",
       "      <td>2.69662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32075</th>\n",
       "      <td>from the pain</td>\n",
       "      <td>-9.6531</td>\n",
       "      <td>-7.02547</td>\n",
       "      <td>2.62763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62901</th>\n",
       "      <td>nothing feels</td>\n",
       "      <td>-9.6531</td>\n",
       "      <td>-7.09958</td>\n",
       "      <td>2.55352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75339</th>\n",
       "      <td>school and do</td>\n",
       "      <td>-9.6531</td>\n",
       "      <td>-7.09958</td>\n",
       "      <td>2.55352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17214</th>\n",
       "      <td>but still did</td>\n",
       "      <td>-9.6531</td>\n",
       "      <td>-7.09958</td>\n",
       "      <td>2.55352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86072</th>\n",
       "      <td>the gut</td>\n",
       "      <td>-9.6531</td>\n",
       "      <td>-7.09958</td>\n",
       "      <td>2.55352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79468</th>\n",
       "      <td>some classes</td>\n",
       "      <td>-9.6531</td>\n",
       "      <td>-7.17963</td>\n",
       "      <td>2.47348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25453</th>\n",
       "      <td>dwelling</td>\n",
       "      <td>-9.6531</td>\n",
       "      <td>-7.17963</td>\n",
       "      <td>2.47348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88640</th>\n",
       "      <td>therapist told me</td>\n",
       "      <td>-9.6531</td>\n",
       "      <td>-7.17963</td>\n",
       "      <td>2.47348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82229</th>\n",
       "      <td>sum up</td>\n",
       "      <td>-9.6531</td>\n",
       "      <td>-7.17963</td>\n",
       "      <td>2.47348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98958</th>\n",
       "      <td>wake up am</td>\n",
       "      <td>-9.6531</td>\n",
       "      <td>-7.17963</td>\n",
       "      <td>2.47348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7887</th>\n",
       "      <td>and since have</td>\n",
       "      <td>-9.6531</td>\n",
       "      <td>-7.17963</td>\n",
       "      <td>2.47348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105789</th>\n",
       "      <td>worried about him</td>\n",
       "      <td>-9.6531</td>\n",
       "      <td>-7.17963</td>\n",
       "      <td>2.47348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16238</th>\n",
       "      <td>broke up for</td>\n",
       "      <td>-9.6531</td>\n",
       "      <td>-7.17963</td>\n",
       "      <td>2.47348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55736</th>\n",
       "      <td>mental abuse</td>\n",
       "      <td>-9.6531</td>\n",
       "      <td>-7.26664</td>\n",
       "      <td>2.38646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27149</th>\n",
       "      <td>every day just</td>\n",
       "      <td>-9.6531</td>\n",
       "      <td>-7.26664</td>\n",
       "      <td>2.38646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73382</th>\n",
       "      <td>relationships as</td>\n",
       "      <td>-9.6531</td>\n",
       "      <td>-7.26664</td>\n",
       "      <td>2.38646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97668</th>\n",
       "      <td>up but the</td>\n",
       "      <td>-9.6531</td>\n",
       "      <td>-7.26664</td>\n",
       "      <td>2.38646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76636</th>\n",
       "      <td>she hurt</td>\n",
       "      <td>-9.6531</td>\n",
       "      <td>-7.26664</td>\n",
       "      <td>2.38646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64065</th>\n",
       "      <td>of it want</td>\n",
       "      <td>-9.6531</td>\n",
       "      <td>-7.26664</td>\n",
       "      <td>2.38646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     term neg_proba_log pos_proba_log log_diff\n",
       "22869      depression she       -9.6531      -6.95648  2.69662\n",
       "32075       from the pain       -9.6531      -7.02547  2.62763\n",
       "62901       nothing feels       -9.6531      -7.09958  2.55352\n",
       "75339       school and do       -9.6531      -7.09958  2.55352\n",
       "17214       but still did       -9.6531      -7.09958  2.55352\n",
       "86072             the gut       -9.6531      -7.09958  2.55352\n",
       "79468        some classes       -9.6531      -7.17963  2.47348\n",
       "25453            dwelling       -9.6531      -7.17963  2.47348\n",
       "88640   therapist told me       -9.6531      -7.17963  2.47348\n",
       "82229              sum up       -9.6531      -7.17963  2.47348\n",
       "98958          wake up am       -9.6531      -7.17963  2.47348\n",
       "7887       and since have       -9.6531      -7.17963  2.47348\n",
       "105789  worried about him       -9.6531      -7.17963  2.47348\n",
       "16238        broke up for       -9.6531      -7.17963  2.47348\n",
       "55736        mental abuse       -9.6531      -7.26664  2.38646\n",
       "27149      every day just       -9.6531      -7.26664  2.38646\n",
       "73382    relationships as       -9.6531      -7.26664  2.38646\n",
       "97668          up but the       -9.6531      -7.26664  2.38646\n",
       "76636            she hurt       -9.6531      -7.26664  2.38646\n",
       "64065          of it want       -9.6531      -7.26664  2.38646"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_imp = pd.DataFrame(data=[feature_names,bnb.feature_log_prob_[0,:],bnb.feature_log_prob_[1,:]]).T\n",
    "feat_imp.columns = [\"term\", \"neg_proba_log\", \"pos_proba_log\"]\n",
    "feat_imp[\"log_diff\"] = feat_imp[\"pos_proba_log\"] - feat_imp[\"neg_proba_log\"]\n",
    "feat_imp.sort_values(\"log_diff\", ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_docfreq = 1\n",
    "ngram_range = (1,2)\n",
    "cv = feature_extraction.text.CountVectorizer(min_df=min_docfreq, ngram_range=ngram_range, binary=False)\n",
    "X_train_cv = cv.fit_transform(X_train)\n",
    "X_test_cv = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.47      0.52      1760\n",
      "           1       0.54      0.64      0.59      1720\n",
      "\n",
      "    accuracy                           0.56      3480\n",
      "   macro avg       0.56      0.56      0.56      3480\n",
      "weighted avg       0.56      0.56      0.55      3480\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mnb = naive_bayes.MultinomialNB()\n",
    "mnb.fit(X_train_cv,y_train)\n",
    "y_pred = mnb.predict(X_test_cv)\n",
    "print((metrics.classification_report(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feat_imp = pd.DataFrame(data=[feature_names,mnb.feature_log_prob_[0,:],mnb.feature_log_prob_[1,:]]).T\n",
    "# feat_imp.columns = [\"term\", \"neg_proba_log\", \"pos_proba_log\"]\n",
    "# feat_imp[\"log_diff\"] = feat_imp[\"pos_proba_log\"] - feat_imp[\"neg_proba_log\"]\n",
    "# # feat_imp.sort_values(\"log_diff\", ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_docfreq = 10\n",
    "ngram_range = (1,3)\n",
    "tfidf = feature_extraction.text.TfidfVectorizer(min_df=min_docfreq, ngram_range=ngram_range, norm='l2')\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.53      0.55      1760\n",
      "           1       0.55      0.59      0.57      1720\n",
      "\n",
      "    accuracy                           0.56      3480\n",
      "   macro avg       0.56      0.56      0.56      3480\n",
      "weighted avg       0.56      0.56      0.56      3480\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = linear_model.LogisticRegression(C=1, penalty=\"l1\", solver=\"saga\")\n",
    "lr.fit(X_train_tfidf,y_train)\n",
    "y_pred = lr.predict(X_test_tfidf)\n",
    "print((metrics.classification_report(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22824</th>\n",
       "      <td>depression</td>\n",
       "      <td>12.2733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22783</th>\n",
       "      <td>depressed</td>\n",
       "      <td>4.58812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19390</th>\n",
       "      <td>christmas</td>\n",
       "      <td>3.26465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100790</th>\n",
       "      <td>way too</td>\n",
       "      <td>2.9667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77101</th>\n",
       "      <td>shit</td>\n",
       "      <td>2.89598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27377</th>\n",
       "      <td>everything</td>\n",
       "      <td>2.80194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27267</th>\n",
       "      <td>everyday</td>\n",
       "      <td>2.79177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46544</th>\n",
       "      <td>it does not</td>\n",
       "      <td>2.71952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56249</th>\n",
       "      <td>miss</td>\n",
       "      <td>2.65793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26535</th>\n",
       "      <td>escape</td>\n",
       "      <td>2.50336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13595</th>\n",
       "      <td>because</td>\n",
       "      <td>2.29276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7150</th>\n",
       "      <td>and made</td>\n",
       "      <td>2.22237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13921</th>\n",
       "      <td>because was</td>\n",
       "      <td>2.21224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83695</th>\n",
       "      <td>thank</td>\n",
       "      <td>2.14469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40879</th>\n",
       "      <td>horror</td>\n",
       "      <td>2.13903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55735</th>\n",
       "      <td>mental</td>\n",
       "      <td>2.08333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27699</th>\n",
       "      <td>exist</td>\n",
       "      <td>2.00246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82529</th>\n",
       "      <td>survive</td>\n",
       "      <td>1.97457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48020</th>\n",
       "      <td>jobs</td>\n",
       "      <td>1.90667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101759</th>\n",
       "      <td>wellbutrin</td>\n",
       "      <td>1.89373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               term     coef\n",
       "22824    depression  12.2733\n",
       "22783     depressed  4.58812\n",
       "19390     christmas  3.26465\n",
       "100790      way too   2.9667\n",
       "77101          shit  2.89598\n",
       "27377    everything  2.80194\n",
       "27267      everyday  2.79177\n",
       "46544   it does not  2.71952\n",
       "56249          miss  2.65793\n",
       "26535        escape  2.50336\n",
       "13595       because  2.29276\n",
       "7150       and made  2.22237\n",
       "13921   because was  2.21224\n",
       "83695         thank  2.14469\n",
       "40879        horror  2.13903\n",
       "55735        mental  2.08333\n",
       "27699         exist  2.00246\n",
       "82529       survive  1.97457\n",
       "48020          jobs  1.90667\n",
       "101759   wellbutrin  1.89373"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_imp = pd.DataFrame(data=[tfidf.get_feature_names(),lr.coef_[0]]).T\n",
    "feat_imp.columns = [\"term\", \"coef\"]\n",
    "feat_imp.sort_values(\"coef\", ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2b8ad5179c5cbf4f014799a3845c0823a20ad2c56229864a15d472d8cf0fb047"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
